{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af0de382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import arxiv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import anthropic\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f216969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "PAPER_DIR = DATA_DIR / \"papers\"\n",
    "\n",
    "Path.mkdir(PAPER_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e8fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied verbatim from course notebook\n",
    "def search_papers(topic: str, max_results: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Search for papers on arXiv based on a topic and store their information.\n",
    "    \n",
    "    Args:\n",
    "        topic: The topic to search for\n",
    "        max_results: Maximum number of results to retrieve (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        List of paper IDs found in the search\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use arxiv to find the papers \n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Search for the most relevant articles matching the queried topic\n",
    "    search = arxiv.Search(\n",
    "        query = topic,\n",
    "        max_results = max_results,\n",
    "        sort_by = arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    papers = client.results(search)\n",
    "    \n",
    "    # Create directory for this topic\n",
    "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(path, \"papers_info.json\")\n",
    "\n",
    "    # Try to load existing papers info\n",
    "    try:\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        papers_info = {}\n",
    "\n",
    "    # Process each paper and add to papers_info  \n",
    "    paper_ids = []\n",
    "    for paper in papers:\n",
    "        paper_ids.append(paper.get_short_id())\n",
    "        paper_info = {\n",
    "            'title': paper.title,\n",
    "            'authors': [author.name for author in paper.authors],\n",
    "            'summary': paper.summary,\n",
    "            'pdf_url': paper.pdf_url,\n",
    "            'published': str(paper.published.date())\n",
    "        }\n",
    "        papers_info[paper.get_short_id()] = paper_info\n",
    "    \n",
    "    # Save updated papers_info to json file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(papers_info, json_file, indent=2)\n",
    "    \n",
    "    print(f\"Results are saved in: {file_path}\")\n",
    "    \n",
    "    return paper_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "578ba44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are saved in: data\\papers\\ai\\papers_info.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2409.12922v1',\n",
       " '2406.11563v3',\n",
       " '2402.07632v3',\n",
       " '2211.05075v1',\n",
       " '2403.15481v2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_papers(\"AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d403bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(paper_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for information about a specific paper across all topic directories.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: The ID of the paper to look for\n",
    "        \n",
    "    Returns:\n",
    "        JSON string with paper information if found, error message if not found\n",
    "    \"\"\"\n",
    " \n",
    "    for item in os.listdir(PAPER_DIR):\n",
    "        item_path = os.path.join(PAPER_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            file_path = os.path.join(item_path, \"papers_info.json\")\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    with open(file_path, \"r\") as json_file:\n",
    "                        papers_info = json.load(json_file)\n",
    "                        if paper_id in papers_info:\n",
    "                            return json.dumps(papers_info[paper_id], indent=2)\n",
    "                except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "                    print(f\"Error reading {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    return f\"There's no saved information related to paper {paper_id}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f5d3272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\\n'\n",
      " '  \"title\": \"AI Thinking: A framework for rethinking artificial intelligence '\n",
      " 'in practice\",\\n'\n",
      " '  \"authors\": [\\n'\n",
      " '    \"Denis Newman-Griffis\"\\n'\n",
      " '  ],\\n'\n",
      " '  \"summary\": \"Artificial intelligence is transforming the way we work with '\n",
      " 'information\\\\nacross disciplines and practical contexts. A growing range of '\n",
      " 'disciplines are\\\\nnow involved in studying, developing, and assessing the '\n",
      " 'use of AI in practice,\\\\nbut these disciplines often employ conflicting '\n",
      " 'understandings of what AI is and\\\\nwhat is involved in its use. New, '\n",
      " 'interdisciplinary approaches are needed to\\\\nbridge competing '\n",
      " 'conceptualisations of AI in practice and help shape the future\\\\nof AI use. '\n",
      " 'I propose a novel conceptual framework called AI Thinking, which\\\\nmodels '\n",
      " 'key decisions and considerations involved in AI use across '\n",
      " 'disciplinary\\\\nperspectives. The AI Thinking model addresses five '\n",
      " 'practice-based competencies\\\\ninvolved in applying AI in context: motivating '\n",
      " 'AI use in information processes,\\\\nformulating AI methods, assessing '\n",
      " 'available tools and technologies, selecting\\\\nappropriate data, and '\n",
      " 'situating AI in the sociotechnical contexts it is used\\\\nin. A hypothetical '\n",
      " 'case study is provided to illustrate the application of AI\\\\nThinking in '\n",
      " 'practice. This article situates AI Thinking in broader\\\\ncross-disciplinary '\n",
      " 'discourses of AI, including its connections to ongoing\\\\ndiscussions around '\n",
      " 'AI literacy and AI-driven innovation. AI Thinking can help\\\\nto bridge '\n",
      " 'divides between academic disciplines and diverse contexts of AI use,\\\\nand '\n",
      " 'to reshape the future of AI in practice.\",\\n'\n",
      " '  \"pdf_url\": \"http://arxiv.org/pdf/2409.12922v1\",\\n'\n",
      " '  \"published\": \"2024-08-26\"\\n'\n",
      " '}')\n"
     ]
    }
   ],
   "source": [
    "pprint(extract_info('2409.12922v1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2046ba54",
   "metadata": {},
   "source": [
    "## Tools Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d0d27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"name\": \"search_papers\",\n",
    "        \"description\": \"Search for papers on arXiv based on a topic and store their information.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"topic\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The topic to search for\"\n",
    "                },\n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of results to retrieve (default: 5)\",\n",
    "                    \"default\": 5\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"topic\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"extract_info\",\n",
    "        \"description\": \"Search for information about a specific paper across all topic directories.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"paper_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The ID of the paper to look for\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"paper_id\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dae3159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling tool execution\n",
    "mapping_tool_function = {\n",
    "    \"search_papers\": search_papers,\n",
    "    \"extract_info\": extract_info\n",
    "}\n",
    "\n",
    "def execute_tool(tool_name: str, args: dict):\n",
    "    result = mapping_tool_function[tool_name](**args)\n",
    "    if result is None:\n",
    "        result = \"The operation completed but didn't return any result.\"\n",
    "    elif isinstance(result, list):\n",
    "        result = \", \".join(result)\n",
    "    elif isinstance(result, dict):\n",
    "        result = json.dumps(result, indent=2)\n",
    "    else:\n",
    "        result = str(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d546ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic()\n",
    "\n",
    "def process_query(query):\n",
    "    \n",
    "    messages = [{'role': 'user', 'content': query}]\n",
    "    \n",
    "    response = client.messages.create(max_tokens = 2024,\n",
    "                                model = 'claude-3-7-sonnet-20250219', \n",
    "                                tools = tools,\n",
    "                                messages = messages)\n",
    "    \n",
    "    process_query = True\n",
    "    while process_query:\n",
    "        assistant_content = []\n",
    "\n",
    "        for content in response.content:\n",
    "            if content.type == 'text':\n",
    "                \n",
    "                print(content.text)\n",
    "                assistant_content.append(content)\n",
    "                \n",
    "                if len(response.content) == 1:\n",
    "                    process_query = False\n",
    "            \n",
    "            elif content.type == 'tool_use':\n",
    "                \n",
    "                assistant_content.append(content)\n",
    "                messages.append({'role': 'assistant', 'content': assistant_content})\n",
    "                \n",
    "                tool_id = content.id\n",
    "                tool_args = content.input\n",
    "                tool_name = content.name\n",
    "                print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                \n",
    "                result = execute_tool(tool_name, tool_args)\n",
    "                messages.append({\"role\": \"user\", \n",
    "                                  \"content\": [\n",
    "                                      {\n",
    "                                          \"type\": \"tool_result\",\n",
    "                                          \"tool_use_id\": tool_id,\n",
    "                                          \"content\": result\n",
    "                                      }\n",
    "                                  ]\n",
    "                                })\n",
    "                response = client.messages.create(max_tokens = 2024,\n",
    "                                  model = 'claude-3-7-sonnet-20250219', \n",
    "                                  tools = tools,\n",
    "                                  messages = messages) \n",
    "                \n",
    "                if len(response.content) == 1 and response.content[0].type == \"text\":\n",
    "                    print(response.content[0].text)\n",
    "                    process_query = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da281fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop():\n",
    "    print(\"Type your queries or 'quit' to exit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nQuery: \").strip()\n",
    "            if query.lower() == 'quit':\n",
    "                break\n",
    "    \n",
    "            process_query(query)\n",
    "            print(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d8f66e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your queries or 'quit' to exit.\n",
      "I'll help you search for papers related to LLMs (Large Language Models). Let me search for relevant papers on arXiv for you.\n",
      "Calling tool search_papers with args {'topic': 'LLM'}\n",
      "Results are saved in: data\\papers\\llm\\papers_info.json\n",
      "I've found 5 papers related to LLMs. Let me get more detailed information about each of these papers for you:\n",
      "Calling tool extract_info with args {'paper_id': '2412.18022v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2406.10300v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2405.19888v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2311.10372v2'}\n",
      "Calling tool extract_info with args {'paper_id': '2411.15764v1'}\n",
      "Based on my search, here are 5 recent papers related to LLMs (Large Language Models):\n",
      "\n",
      "1. **Trustworthy and Efficient LLMs Meet Databases** (December 2024)\n",
      "   - Focuses on making LLMs more trustworthy and efficient, particularly in reducing hallucinations and meeting high inference demands\n",
      "   - Explores the intersection between LLMs and databases, highlighting new opportunities and challenges\n",
      "\n",
      "2. **Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications** (June 2024)\n",
      "   - Provides a taxonomy for LLM-integrated applications \n",
      "   - Identifies thirteen dimensions to characterize LLM components in applications\n",
      "   - Offers a framework for analyzing and describing systems that leverage LLMs\n",
      "\n",
      "3. **Parrot: Efficient Serving of LLM-based Applications with Semantic Variable** (May 2024)\n",
      "   - Introduces \"Parrot,\" a system focused on the end-to-end experience of LLM-based applications\n",
      "   - Proposes \"Semantic Variable\" as a unified abstraction to expose application-level knowledge to LLM services\n",
      "   - Demonstrates significant performance improvements for LLM applications\n",
      "\n",
      "4. **A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends** (November 2023)\n",
      "   - Comprehensive survey on specialized LLMs for software engineering (Code LLMs)\n",
      "   - Examines the relationship between general LLMs and Code LLMs\n",
      "   - Evaluates performance differences across various software engineering tasks\n",
      "\n",
      "5. **LLM Online Spatial-temporal Signal Reconstruction Under Noise** (November 2024)\n",
      "   - Introduces the LLM-OSR framework integrating Graph Signal Processing and LLMs\n",
      "   - Focuses on online spatial-temporal signal reconstruction\n",
      "   - Demonstrates robustness under Gaussian noise conditions using GPT-4-o mini\n",
      "\n",
      "Would you like more specific information about any of these papers or would you like me to search for papers on a more specific aspect of LLMs?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358bfda",
   "metadata": {},
   "source": [
    "## Creating an MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e30f059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
